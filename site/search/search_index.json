{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Math Handbook","text":"<p>A Markdown-first math handbook organized as flashcards.</p>"},{"location":"#dynamic-indextags-mkdocs","title":"Dynamic index/tags + MkDocs","text":"<p>Everything is generated from card frontmatter:</p> <pre><code>./run.sh\n</code></pre> <p>This runs: - <code>python scripts/build_index.py</code> (regenerates <code>index/INDEX.md</code>, <code>index/TAGS.md</code>, and <code>mkdocs.yml</code>) - <code>mkdocs serve</code> (or any <code>mkdocs</code> subcommand you pass)</p> <p>Examples:</p> <pre><code>./run.sh build\n./run.sh gh-deploy\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"cards/graphs/modularity/","title":"Modularity","text":"","tags":["graphs","community-detection"]},{"location":"cards/graphs/modularity/#formula","title":"Formula","text":"<p>For an undirected graph with adjacency \\(A\\), degrees \\(k_i\\), total edges \\(m=\\frac{1}{2}\\sum_{ij}A_{ij}\\), and community assignment \\(c_i\\): [ Q = \\frac{1}{2m}\\sum_{i,j}\\left(A_{ij} - \\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}[c_i=c_j] ]</p>","tags":["graphs","community-detection"]},{"location":"cards/graphs/modularity/#what-it-means","title":"What it means","text":"<p>Compares actual within-community edges to the expected count under a degree-preserving \u201cnull model\u201d.</p>","tags":["graphs","community-detection"]},{"location":"cards/graphs/modularity/#parameters","title":"Parameters","text":"<ul> <li>\\(A_{ij}\\): edge weight between \\(i\\) and \\(j\\) (0/1 if unweighted)</li> <li>\\(k_i\\): degree (or weighted degree/strength)</li> <li>\\(m\\): number of edges (or total weight / 2)</li> <li>\\(c_i\\): community label</li> </ul>","tags":["graphs","community-detection"]},{"location":"cards/graphs/modularity/#key-properties","title":"Key properties","text":"<ul> <li>Higher \\(Q\\) suggests stronger community structure</li> <li>Often optimized by Louvain/Leiden heuristics</li> </ul>","tags":["graphs","community-detection"]},{"location":"cards/graphs/modularity/#common-gotchas","title":"Common gotchas","text":"<ul> <li>Resolution limit: modularity can miss small communities in large graphs</li> <li>There are variants with a resolution parameter \\(\\gamma\\):   [   A_{ij} - \\gamma\\frac{k_i k_j}{2m}   ]</li> </ul>","tags":["graphs","community-detection"]},{"location":"cards/info-theory/entropy-shannon/","title":"Shannon Entropy","text":"","tags":["information-theory","probability"]},{"location":"cards/info-theory/entropy-shannon/#formula","title":"Formula","text":"\\[ H(X) = -\\sum_{x} p(x)\\,\\log p(x) \\]","tags":["information-theory","probability"]},{"location":"cards/info-theory/entropy-shannon/#what-it-means","title":"What it means","text":"<p>Expected uncertainty (\u201caverage surprise\u201d) of outcomes of \\(X\\).</p>","tags":["information-theory","probability"]},{"location":"cards/info-theory/entropy-shannon/#parameters","title":"Parameters","text":"<ul> <li>\\(p(x)\\): probability of outcome \\(x\\)</li> <li>log base 2 \u2192 bits; base \\(e\\) \u2192 nats</li> </ul>","tags":["information-theory","probability"]},{"location":"cards/info-theory/entropy-shannon/#key-properties","title":"Key properties","text":"<ul> <li>\\(0 \\le H(X) \\le \\log |\\mathcal{X}|\\) for finite \\(\\mathcal{X}\\)</li> <li>Maximized by uniform distribution</li> <li>\\(H(X)=0\\) if \\(X\\) is deterministic</li> </ul>","tags":["information-theory","probability"]},{"location":"cards/info-theory/entropy-shannon/#common-gotchas","title":"Common gotchas","text":"<ul> <li>Don\u2019t mix log bases when comparing numeric values.</li> <li>Continuous version is differential entropy (behaves differently).</li> </ul>","tags":["information-theory","probability"]},{"location":"cards/info-theory/entropy-shannon/#quick-example","title":"Quick example","text":"<p>For \\(p=[0.5, 0.5]\\), \\(H=1\\) bit (base 2).</p>","tags":["information-theory","probability"]},{"location":"cards/info-theory/kl-divergence/","title":"KL Divergence (Relative Entropy)","text":"","tags":["information-theory","probability","optimization"]},{"location":"cards/info-theory/kl-divergence/#formula","title":"Formula","text":"<p>Discrete: [ D_{\\mathrm{KL}}(P|Q) = \\sum_x p(x)\\,\\log\\frac{p(x)}{q(x)} ]</p> <p>Continuous: [ D_{\\mathrm{KL}}(P|Q) = \\int p(x)\\,\\log\\frac{p(x)}{q(x)}\\,dx ]</p>","tags":["information-theory","probability","optimization"]},{"location":"cards/info-theory/kl-divergence/#what-it-means","title":"What it means","text":"<p>\u201cHow many extra nats/bits\u201d you spend coding samples from \\(P\\) using a code optimized for \\(Q\\).</p>","tags":["information-theory","probability","optimization"]},{"location":"cards/info-theory/kl-divergence/#parameters","title":"Parameters","text":"<ul> <li>\\(P\\): true distribution (often \u201cdata\u201d)</li> <li>\\(Q\\): approximate / model distribution</li> </ul>","tags":["information-theory","probability","optimization"]},{"location":"cards/info-theory/kl-divergence/#key-properties","title":"Key properties","text":"<ul> <li>\\(D_{\\mathrm{KL}}(P\\|Q) \\ge 0\\) (equals 0 iff \\(P=Q\\) a.e.)</li> <li>Not symmetric: \\(D_{\\mathrm{KL}}(P\\|Q) \\ne D_{\\mathrm{KL}}(Q\\|P)\\)</li> <li>Not a metric (no triangle inequality)</li> </ul>","tags":["information-theory","probability","optimization"]},{"location":"cards/info-theory/kl-divergence/#relation-to-cross-entropy","title":"Relation to cross-entropy","text":"<p>[ H(P,Q) = H(P) + D_{\\mathrm{KL}}(P|Q) ] So minimizing cross-entropy over \\(Q\\) is equivalent to minimizing KL (since \\(H(P)\\) is fixed).</p>","tags":["information-theory","probability","optimization"]},{"location":"cards/ml-metrics/auc-roc/","title":"ROC AUC","text":"","tags":["ml","metrics","ranking"]},{"location":"cards/ml-metrics/auc-roc/#definition","title":"Definition","text":"<p>AUC is the area under the ROC curve (TPR vs FPR) as you sweep a score threshold.</p>","tags":["ml","metrics","ranking"]},{"location":"cards/ml-metrics/auc-roc/#equivalent-probabilistic-view","title":"Equivalent probabilistic view","text":"<p>[ \\mathrm{AUC} = P(s(x^+) &gt; s(x^-)) + \\tfrac{1}{2}P(s(x^+)=s(x^-)) ] i.e., probability a random positive is ranked above a random negative (ties count half).</p>","tags":["ml","metrics","ranking"]},{"location":"cards/ml-metrics/auc-roc/#key-properties","title":"Key properties","text":"<ul> <li>Threshold-independent ranking metric</li> <li>Invariant to any strictly monotone transform of scores</li> </ul>","tags":["ml","metrics","ranking"]},{"location":"cards/ml-metrics/auc-roc/#common-gotchas","title":"Common gotchas","text":"<ul> <li>AUC can look \u201cgood\u201d under extreme class imbalance even when precision is poor</li> <li>If you care about top-of-list performance, consider PR AUC / precision@k</li> </ul>","tags":["ml","metrics","ranking"]},{"location":"cards/ml-metrics/jaccard/","title":"Jaccard Similarity (Intersection over Union)","text":"","tags":["ml","metrics","similarity"]},{"location":"cards/ml-metrics/jaccard/#formula-sets","title":"Formula (sets)","text":"\\[ J(A,B) = \\frac{|A\\cap B|}{|A\\cup B|} \\]","tags":["ml","metrics","similarity"]},{"location":"cards/ml-metrics/jaccard/#formula-binary-vectors-segmentation","title":"Formula (binary vectors / segmentation)","text":"<p>If TP/FP/FN are counts: [ J = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}} ]</p>","tags":["ml","metrics","similarity"]},{"location":"cards/ml-metrics/jaccard/#what-it-means","title":"What it means","text":"<p>Overlap ratio between predicted and true items.</p>","tags":["ml","metrics","similarity"]},{"location":"cards/ml-metrics/jaccard/#relation-to-dice-f1-binary","title":"Relation to Dice / F1 (binary)","text":"<p>Dice: [ D = \\frac{2\\mathrm{TP}}{2\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}} ] Convert: [ D = \\frac{2J}{1+J},\\quad J=\\frac{D}{2-D} ]</p>","tags":["ml","metrics","similarity"]},{"location":"cards/ml-metrics/jaccard/#common-gotchas","title":"Common gotchas","text":"<ul> <li>Undefined when both sets empty; choose a convention (often 1.0).</li> </ul>","tags":["ml","metrics","similarity"]},{"location":"cards/ml-metrics/log-loss/","title":"Log Loss (Binary Cross-Entropy)","text":"","tags":["ml","metrics","classification","likelihood"]},{"location":"cards/ml-metrics/log-loss/#formula","title":"Formula","text":"<p>Given labels \\(y_i\\in\\{0,1\\}\\) and predicted probabilities \\(\\hat p_i = P(y_i=1)\\): [ \\mathrm{LogLoss} = -\\frac{1}{n}\\sum_{i=1}^n \\left[y_i\\log(\\hat p_i) + (1-y_i)\\log(1-\\hat p_i)\\right] ]</p>","tags":["ml","metrics","classification","likelihood"]},{"location":"cards/ml-metrics/log-loss/#what-it-means","title":"What it means","text":"<p>Penalizes confident wrong predictions heavily; corresponds to Bernoulli negative log-likelihood.</p>","tags":["ml","metrics","classification","likelihood"]},{"location":"cards/ml-metrics/log-loss/#parameters","title":"Parameters","text":"<ul> <li>\\(y_i\\): true label</li> <li>\\(\\hat p_i\\): predicted probability for class 1</li> </ul>","tags":["ml","metrics","classification","likelihood"]},{"location":"cards/ml-metrics/log-loss/#key-properties","title":"Key properties","text":"<ul> <li>Proper scoring rule: minimized in expectation by predicting the true probability</li> <li>Unbounded above (if you predict 0 for a true 1 \u2192 \\(-\\log 0\\))</li> </ul>","tags":["ml","metrics","classification","likelihood"]},{"location":"cards/ml-metrics/log-loss/#common-gotchas","title":"Common gotchas","text":"<ul> <li>Clip probabilities: \\(\\hat p \\leftarrow \\mathrm{clip}(\\hat p, \\epsilon, 1-\\epsilon)\\)</li> <li>Don\u2019t confuse with accuracy: log loss cares about calibration</li> </ul>","tags":["ml","metrics","classification","likelihood"]},{"location":"cards/ml-metrics/negative-log-likelihood/","title":"Negative Log-Likelihood (NLL)","text":"","tags":["ml","likelihood","optimization"]},{"location":"cards/ml-metrics/negative-log-likelihood/#formula","title":"Formula","text":"<p>For model \\(p_\\theta(x)\\) and samples \\(x_1,\\dots,x_n\\): [ \\mathrm{NLL}(\\theta) = -\\sum_{i=1}^n \\log p_\\theta(x_i) ] Often averaged: \\(-\\frac{1}{n}\\sum_i \\log p_\\theta(x_i)\\).</p>","tags":["ml","likelihood","optimization"]},{"location":"cards/ml-metrics/negative-log-likelihood/#what-it-means","title":"What it means","text":"<p>Training by maximum likelihood is minimizing NLL.</p>","tags":["ml","likelihood","optimization"]},{"location":"cards/ml-metrics/negative-log-likelihood/#examples","title":"Examples","text":"<ul> <li>Bernoulli (binary classification) \u2192 NLL equals binary log loss</li> <li>Categorical (multiclass) \u2192 NLL equals multiclass cross-entropy</li> <li>Gaussian regression with fixed \\(\\sigma\\):   [   -\\log \\mathcal{N}(y\\mid \\mu_\\theta(x), \\sigma^2)   \\propto \\frac{(y-\\mu_\\theta(x))^2}{2\\sigma^2}   ]   So MSE is a special case (up to constants/scaling).</li> </ul>","tags":["ml","likelihood","optimization"]},{"location":"cards/ml-metrics/negative-log-likelihood/#common-gotchas","title":"Common gotchas","text":"<ul> <li>Make sure \\(p_\\theta\\) is a valid normalized probability density/mass.</li> <li>For densities, values can exceed 1; \\(\\log p\\) is still valid.</li> </ul>","tags":["ml","likelihood","optimization"]},{"location":"index/GLOSSARY/","title":"Glossary","text":"<ul> <li>logit: The inverse of the sigmoid. For probability \\(p\\), \\(\\mathrm{logit}(p)=\\log\\frac{p}{1-p}\\).</li> <li>sigmoid: The logistic function \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\), mapping \\(\\mathbb{R}\\to(0,1)\\).</li> <li>softmax: Normalizes a vector \\(z\\) into a categorical distribution: \\(\\mathrm{softmax}(z_i)=\\frac{e^{z_i}}{\\sum_j e^{z_j}}\\).</li> </ul>"},{"location":"index/INDEX/","title":"Index","text":"<p>This file is auto-generated. Do not edit by hand.</p>"},{"location":"index/INDEX/#graphs","title":"Graphs","text":"<ul> <li>Modularity (Community Quality)</li> </ul>"},{"location":"index/INDEX/#information-theory","title":"Information Theory","text":"<ul> <li>KL Divergence (Relative Entropy)</li> <li>Shannon Entropy</li> </ul>"},{"location":"index/INDEX/#ml-metrics","title":"ML Metrics","text":"<ul> <li>Jaccard Similarity / IoU</li> <li>Log Loss (Binary Cross-Entropy)</li> <li>Negative Log-Likelihood (NLL)</li> <li>ROC AUC</li> </ul>"},{"location":"index/TAGS/","title":"Tags","text":"<p>This file is auto-generated. Do not edit by hand.</p>"},{"location":"index/TAGS/#classification","title":"classification","text":"<ul> <li>Log Loss (Binary Cross-Entropy)</li> </ul>"},{"location":"index/TAGS/#community-detection","title":"community-detection","text":"<ul> <li>Modularity (Community Quality)</li> </ul>"},{"location":"index/TAGS/#graphs","title":"graphs","text":"<ul> <li>Modularity (Community Quality)</li> </ul>"},{"location":"index/TAGS/#information-theory","title":"information-theory","text":"<ul> <li>KL Divergence (Relative Entropy)</li> <li>Shannon Entropy</li> </ul>"},{"location":"index/TAGS/#likelihood","title":"likelihood","text":"<ul> <li>Log Loss (Binary Cross-Entropy)</li> <li>Negative Log-Likelihood (NLL)</li> </ul>"},{"location":"index/TAGS/#metrics","title":"metrics","text":"<ul> <li>Jaccard Similarity / IoU</li> <li>Log Loss (Binary Cross-Entropy)</li> <li>ROC AUC</li> </ul>"},{"location":"index/TAGS/#ml","title":"ml","text":"<ul> <li>Jaccard Similarity / IoU</li> <li>Log Loss (Binary Cross-Entropy)</li> <li>Negative Log-Likelihood (NLL)</li> <li>ROC AUC</li> </ul>"},{"location":"index/TAGS/#optimization","title":"optimization","text":"<ul> <li>KL Divergence (Relative Entropy)</li> <li>Negative Log-Likelihood (NLL)</li> </ul>"},{"location":"index/TAGS/#probability","title":"probability","text":"<ul> <li>KL Divergence (Relative Entropy)</li> <li>Shannon Entropy</li> </ul>"},{"location":"index/TAGS/#ranking","title":"ranking","text":"<ul> <li>ROC AUC</li> </ul>"},{"location":"index/TAGS/#similarity","title":"similarity","text":"<ul> <li>Jaccard Similarity / IoU</li> </ul>"}]}